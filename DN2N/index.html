<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DN2N">
  <meta name="keywords" content="...">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DN2N</title>
 
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="offcanvas.css" rel="stylesheet">
	
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <title>Text-driven Editing of 3D Scenes without Retraining</title>
 
</head>
	
<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
	<h1 class="nerf_title_v2">DN2N</h1>
	<!--<h3 align="center" class="title is-4"> (Oral at AAAI 2023) </h3>-->
	<h3 class="nerf_subheader_v2">Text-driven Editing of 3D Scenes without Retraining</h3>
    <hr>
    <p class="authors">
        <a href="http://sk-fun.fun/"> Shuangkang Fang</a>,
        <a href="https://shi.buaa.edu.cn/wangyufeng1/zh_CN/index.htm"> Yufeng Wang</a>,
        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=5u95wzMAAAAJ"> Yi Yang</a>,
        <a href="https://sites.google.com/site/yihsuantsai/"> Yi-Hsuan Tsai</a>,
        <a href="https://shi.buaa.edu.cn/dingwenrui/zh_CN/index/127041/list/index.htm"> Wenrui Ding</a>,
	<a href="http://faculty.ucmerced.edu/mhyang/"> Ming-Hsuan Yang</a>,
	<a href="https://zsc.github.io/"> Shuchang Zhou</a>
    </p>

    <hr>
    <p class="authors">
	 Beihang University & </a>,
         Megvii & </a>,
         Google & </a>,
         University of California, Merced & </a>,
         Yonsei University </a>
    </p>
	

<!-- 
<div class="columns is-centered">
    <div class="column is-two-thirds">
        <div class="content has-text-justified has-text-centered">
            <p>
                Beihang University & Megvii & Google & University of California, Merced & Yonsei University
            </p>
        </div>
    </div>
</div>
-->
	
    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div> -->

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2309.04917">Paper</a>
	<a class="btn btn-primary" href="https://github.com/megvii-research/DN2N">Code(coming soon)</a>
    </div>
</div>

	
</section>
<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Numerous diffusion models have recently been applied to image synthesis and editing. 
However, editing 3D scenes is still in its early stages. It poses various challenges, such as the requirement to design specific methods for different editing types, retraining new models for various 3D scenes, and the absence of convenient human interaction during editing.
To tackle these issues, we introduce a text-driven editing method, termed DN2N, which allows for the direct acquisition of a NeRF model with universal editing capabilities, eliminating the requirement for retraining. 
Our method employs off-the-shelf text-based editing models of 2D images to modify the 3D scene images, followed by a filtering process to discard poorly edited images that disrupt 3D consistency. 
We then consider the remaining inconsistency as a problem of removing noise perturbation, which can be solved by generating training data with similar perturbation characteristics for training.
We further propose cross-view regularization terms to help the generalized NeRF model mitigate these perturbations.
Our text-driven method allows users to edit a 3D scene with their desired description, which is more friendly, intuitive, and practical than prior works.
Empirical results show that our method achieves multiple editing types, including but not limited to appearance editing, weather transition, material changing, and style transfer. 
Most importantly, our method generalizes well with editing abilities shared among a set of model parameters without requiring a customized editing model for some specific scenes, thus inferring novel views with editing effects directly from user input.
          </p>

        </div>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->
	
</br></br>

<div class="grey_container w-container">
	<!-- -->
	<h2 class="title is-2">Method</h2>
	<p class="paragraph-3 nerf_text">
We initially utilized a 2D editing model to perform the preliminary editing on the images of a 3D scene. We subsequently apply a designed content filter to remove images with poor editing results that cause significant 3D inconsistency.
However, the remaining images after selection may still contain inconsistent 3D results, in which we consider as noise perturbation to the consistent edited images due to the inherent stochastic and diverse nature of the 2D editing model.
Thus, we leverage this characteristic to create training data pairs by generating image captions through the BLIP model and target captions via GPT, then applying minor perturbations associated with these captions to a 3D scene.
Therefore, these perturbations can be viewed as noise, as well as unedited images as pseudo-ground truth.
Based on this produced training data, we introduce two cross-view regularization terms during training, including the self and neighboring views, to improve the 3D editing consistency. 
The former requires the NeRF model to generate consistent results for the same target view that derives from two different source views, while the latter enforces the overlapping pixel values between the target and adjacent views to be approximately close.
Finally, both the perturbation dataset and regularization terms are incorporated into our generalizable NeRF model training to facilitate its 3D consistency. 
	</p>
	<div align="center" style="margin-top:80px;" style="margin-bottom:120px;">
		<img style='height: auto; width: 90%; object-fit: contain' src="static/images/pipeline.png" alt="overview_image">
	</div> 	
</div>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 align="center"  class="title is-2">Video Display</h2>

        <!-- Mutual-Conversion -->
	<h3 align="center" class="title is-4"> - </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="70%"
		 height="70%">
            <source src="static/videos/flower.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!-- Mutual-Conversion -->
	      
	 <!-- edit-Conversion -->
	<h3 align="center" class="title is-4"> - </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="70%"
		 height="70%">
            <source src="static/videos/fang.mp4"
                    type="video/mp4">
          </video>
        </div>

	<!-- edit-Conversion -->
	<h3 align="center" class="title is-4"> - </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="70%"
		 height="70%">
            <source src="static/videos/others.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
       <h2 class="title">BibTeX</h2>
        <!--<h2 class="title is-2">Abstract</h2>-->
        <div class="content has-text-justified">
          <p>
  @article{DN2N,<br>
  author    = {Fang, Shuangkang and Wang, Yufeng and Yang, Yi and Yang, Yi and Tsai, Yi-Hsuan and Ding, Wenrui and Yang, Ming-Hsuan and Zhou, Shuchang},<br>
  title     = {Text-driven Editing of 3D Scenes without Retraining},<br>
  journal   = {Arxiv},<br>
  year      = {2023}<br>
}
         </p>
        </div>
      </div>
    </div>
  </div>
</section>
    

<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
</footer>

</body>
</html>
