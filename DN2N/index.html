<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DN2N">
  <meta name="keywords" content="...">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DN2N</title>
 
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="offcanvas.css" rel="stylesheet">
	
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <title>Text-driven Editing of 3D Scenes without Retraining</title>
 
</head>
	
<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
	<h1 class="nerf_title_v2">DN2N</h1>
	<!--<h3 align="center" class="title is-4"> (Oral at AAAI 2023) </h3>-->
	<h3 class="nerf_subheader_v2">Text-driven Editing of 3D Scenes without Retraining</h3>
    <hr>
    <p class="authors">
        <a href="https://github.io/"> Shuangkang Fang</a>,
        <a href="http://github.io/"> Yufeng Wang</a>,
        <a href="http://github.io/"> Yi Yang</a>,
        <a href="https://github.io/"> Yi-Hsuan Tsai</a>,
        <a href="https://github.io/"> Wenrui Ding</a>,
	    <a href="https://github.io/"> Ming-Hsuan Yang</a>,
	<a href="https://zsc.github.io/"> Shuchang Zhou</a>
    </p>

    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div> -->

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2211.15977">Paper-PVD</a>
	<a class="btn btn-primary" href="https://arxiv.org/abs/2304.04012">Paper-PVDAL</a>
        <!--<a class="btn btn-primary" href="review.pdf">Supplementary</a>-->
	<a class="btn btn-primary" href="https://github.com/megvii-research/AAAI2023-PVD">Code</a>
    </div>
</div>

<!--
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PVD Under Review</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://skfang.github.io">Shuangkang Fang</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a>Weixin Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Heng Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Yi Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Yufeng Wang</a><sup>2*</sup>,</span>
            <span class="author-block">                
              <a href="https://zsc.github.io/">Shuchang Zhou</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Megvii &nbsp;&nbsp;</span>
            <span class="author-block"><sup>2</sup>Beihang University &nbsp;&nbsp;</span>
          </div>
          <h1 style="font-size:24px;font-weight:bold">AAAI Under Review</h1>
          <div class="column has-text-centered">
            <div class="publication-links">
             
              <span class="link-block">
                <a href="static/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
		    
              <span class="link-block">
                <a href="http://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
               <span class="link-block">
                <a href="static/supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supp</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
-->
	
</section>
<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Numerous diffusion models have recently been applied to image synthesis and editing. 
However, editing 3D scenes is still in its early stages. It poses various challenges, such as the requirement to design specific methods for different editing types, retraining new models for various 3D scenes, and the absence of convenient human interaction during editing.
To tackle these issues, we introduce a text-driven editing method, termed DN2N, which allows for the direct acquisition of a NeRF model with universal editing capabilities, eliminating the requirement for retraining. 
Our method employs off-the-shelf text-based editing models of 2D images to modify the 3D scene images, followed by a filtering process to discard poorly edited images that disrupt 3D consistency. 
We then consider the remaining inconsistency as a problem of removing noise perturbation, which can be solved by generating training data with similar perturbation characteristics for training.
We further propose cross-view regularization terms to help the generalized NeRF model mitigate these perturbations.
Our text-driven method allows users to edit a 3D scene with their desired description, which is more friendly, intuitive, and practical than prior works.
Empirical results show that our method achieves multiple editing types, including but not limited to appearance editing, weather transition, material changing, and style transfer. 
Most importantly, our method generalizes well with editing abilities shared among a set of model parameters without requiring a customized editing model for some specific scenes, thus inferring novel views with editing effects directly from user input.
          </p>

        </div>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->

	<!--           
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                We present a novel approach to model and reconstruct radiance fields.
                Unlike NeRF that uses pure MLPs,
                we consider the full volume field as a 4D tensor and propose to factorize the tensor into multiple compact low-rank tensor components for efficient scene modeling.
            </p>
            <div class="columns-5 w-row">
                <img src="img/teaser_v6.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
   
            <p class="paragraph-3 nerf_text">
                We model a scene (left) as a tensorial radiance field (right) using a set of vectors and matrices that describe scene appearance
                and geometry along their corresponding axes. These vector/matrix factors are used to compute volume density and
                view-dependent RGB color via vector-matrix outer products, leading to efficient radiance field reconstruction and realistic rendering.
                </br>
                We demonstrate that TensoRF with CP decomposition can achieve fast reconstruction with better rendering quality and even a smaller model size (<b>< 4MB</b>) than NeRF.
                Moreover, TensoRF with VM decomposition can further boost our rendering quality to outperform previous state-of-the-art methods and reduce the reconstruction time (<b>< 10min</b> only with standard PyTorch implementation).
            </p>     
            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                We factorize radiance fields into compact components for scene modeling.
                To doso, we apply both the classic CP decomposition and a new vector-matrix (VM) decomposition; both are illustrated in following figure:
            </p>
            <div class="columns-5 w-row">
                <img src="img/tensor_factorizationpng.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
            <p class="paragraph-3 nerf_text">
                Left: CP decomposition, which factorizes atensor as a sum of vector outer products.
                Right: our vector-matrix decomposition, which factorizes a tensor as a sum of vector-matrix outer products.
                Please refer to our paper for more decomposition derails.
            </p>


            <div class="columns-5 w-row">
                <img src="img/pipeline.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text">
                We now present our TensoRF representation and reconstruction. For each shading location <b>x</b> = (x,y,z), we use linearly/bilinearly sampled values from the vector (<b>v</b>)/matrix (<b>M</b>) factors to compute the corresponding trilinearly interpolated values of the tensor components.
                The density component values (A<sub>σ</sub>(x)) are summed to get the volume density directly (σ).
                The appearance values (A<sub>c</sub>(x)) are concatenated into a vector (⊕[A<sub>c</sub><sup style="margin-left:-8px">m</sup>(x)]<sub>m</sub>) that is then multiplied by an appearance matrix (<b>B</b>) and sent to the decoding function S for RGB color (c) regression.
                The decoding function S can be a Spherical Harmonic (SH) function or a fully-connected network (FCN).
            </p>
        </div>
    </div>
-->
	
</br></br>

<div class="grey_container w-container">
	<!-- -->
	<h2 class="title is-2">Method</h2>
	<p class="paragraph-3 nerf_text">
We initially utilized a 2D editing model to perform the preliminary editing on the images of a 3D scene. We subsequently apply a designed content filter to remove images with poor editing results that cause significant 3D inconsistency.
However, the remaining images after selection may still contain inconsistent 3D results, in which we consider as noise perturbation to the consistent edited images due to the inherent stochastic and diverse nature of the 2D editing model.
Thus, we leverage this characteristic to create training data pairs by generating image captions through the BLIP model and target captions via GPT, then applying minor perturbations associated with these captions to a 3D scene.
Therefore, these perturbations can be viewed as noise, as well as unedited images as pseudo-ground truth.
Based on this produced training data, we introduce two cross-view regularization terms during training, including the self and neighboring views, to improve the 3D editing consistency. 
The former requires the NeRF model to generate consistent results for the same target view that derives from two different source views, while the latter enforces the overlapping pixel values between the target and adjacent views to be approximately close.
Finally, both the perturbation dataset and regularization terms are incorporated into our generalizable NeRF model training to facilitate its 3D consistency. 
	</p>
	<div align="center" style="margin-top:80px;" style="margin-bottom:120px;">
		<img style='height: auto; width: 125%; object-fit: contain' src="static/images/pipeline.png" alt="overview_image">
	</div> 	
</div>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 align="center"  class="title is-2">Video Display</h2>

        <!-- Mutual-Conversion -->
        <h3 align="center" class="title is-4">Mutual-Conversion - lego </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="53%"
		 height="53%">
            <source src="static/videos/video_mutual_lego_15fps.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!-- Mutual-Conversion -->
	      
	 <!-- edit-Conversion -->
        <h3 align="center" class="title is-4">Erasing the bucket by editing Tensors(Plenoxels) and distilling back to MLP(NeRF) - lego </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="43%"
		 height="43%">
            <source src="static/videos/edit_video.mp4"
                    type="video/mp4">
          </video>
        </div>

	<!-- Hash2others -->
        <h3 align="center" class="title is-4">VM2others - LLFF </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="63%"
		 height="63%">
            <source src="static/videos/VM2others_llff_fps25_withTea.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!-- Hash2others -->
	  
	 <!-- Hash2others tanks -->
        <h3 align="center" class="title is-4">Hash2others - TanksAndTemples </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster=""
                 id="replay-video"
		 autoplay
                 controls
		 muted
                 loop
                 width="90%"
		 height="90%">
            <source src="static/videos/hash2others-1.2mul-tanks-withTea.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!-- Hash2others tanks-->
	      
        <!-- Ours Finetuned vs. NeRF
        <h3 align="center" style="margin-top:80px;"  class="title is-4">Ingp2Nerf vs. NeRF </h3>
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video poster='static/images/nerf_ours_ft_preview.png'
                 id="replay-video"
                 controls
                 muted
                 width="75%">
            <source src="static/videos/video_mutual_lego_15fps.mp4"
                    type="video/mp4">
          </video>
        </div>
	-->
          
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
       <h2 class="title">BibTeX</h2>
        <!--<h2 class="title is-2">Abstract</h2>-->
        <div class="content has-text-justified">
          <p>
  @article{pvd2023,<br>
  author    = {Fang, Shuangkang and Xu, Weixin and Wang, Heng and Yang, Yi and Wang, Yufeng and Zhou, Shuchang},<br>
  title     = {One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation},<br>
  journal   = {AAAI},<br>
  year      = {2023}<br>
}
         </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code,align="left">@article{pvd2023,
  author    = {Fang, Shuangkang and Xu, Weixin and Wang, Heng and Yang, Yi and Wang, Yufeng and Zhou, Shuchang},
  title     = {One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation},
  journal   = {AAAI},
  year      = {2023},
}</code></pre>
  </div>
</section>-->

<!--
<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{pvd2023,
  author={Fang, Shuangkang and Xu, Weixin and Wang, Heng and Yang, Yi and Wang, Yufeng and Zhou, Shuchang},
  title={One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation},
  booktitle={AAAI},
  year={2023}
}
</code></pre>
  </div>
</section>
-->
    

<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
</footer>

</body>
</html>
